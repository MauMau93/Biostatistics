---
title: "Biostatistics - Task  1"
author: "Adrian White & Mauricio Marcos Fajgenbaun"
output: html_document
---


```{r}
library(dgof)
library(ggplot2)
library (wPerm)
library(coin)
library(stats)
library (broman)
library( vcd )
library( dplyr )
library( mosaic )
```


## Exercise 1

First, we need to create a data frame with the values of the table.

```{r}
x <- data.frame("Density" = c("None","Low","Intermediate","High"), "Mean_Seagrass" = c(34.81,33.13,28.33,15.00))
x
x$Density <- as.factor(x$Density);
x$Density
```

a) Three Bonferroni adjusted confidence interval, comparing each parcel with the parcel that had no oysters introduced.

In general, a confidence interval testing equal mean between groups can be performed as follows.

```{r}
library(knitr)
```


```{r}
mu_none <- 34.81
mu_low <- 33.13
mu_int <- 28.33
mu_high <- 15.00
MSE <- 220.94
n <- 10
N <- 40
k = 4
alpha <- 0.05
comp <- 3
alpha_bonf <- alpha/comp
t <- pt(1-alpha_bonf/2,N-k)
```

Now, we can compute the significance level, adjusted by the bonferroni method: 

```{r}
bonf_confid <- (1-(alpha_bonf*2))
bonf_confid
```

i) Confidence Interval between means: low vs none

```{r}
low <- (mu_none-mu_low) - t*sqrt((1/n)+(1/n))
upp <- (mu_none-mu_low) + t*sqrt((1/n)+(1/n))
low
upp 
```
This means that a bonferroni adjusted confidence interval at 96.66667% of confidence for the mean difference between the control parcel and the one with low density of oysters is: (1.3075 , 2.0524).

Let´s repeat the same proceadure for the other two comparisons:


ii) Confidence Interval between means: intermediate vs none

```{r}
low <- (mu_none-mu_int) - t*sqrt((1/n)+(1/n))
upp <- (mu_none-mu_int) + t*sqrt((1/n)+(1/n))
low
upp
```

iii) Confidence Interval between means: high vs none

```{r}
low <- (mu_none-mu_high) - t*sqrt((1/n)+(1/n))
upp <- (mu_none-mu_high) + t*sqrt((1/n)+(1/n))
low
upp
```

b) Repeat the previous item with the Tukey’s Honest Significant Difference
`
Now, instead of using a t-statistic, we will be using a studentized range distribution. The confidence interval with the Tukey´s method is as follows:


```{r}
q <- ptukey(q = 0.95 , nmeans = 10, df = 36)
q
```

i) Confidence Interval between means: low vs none

```{r}
low <- (mu_none - mu_low)-(1/sqrt(2))*q*MSE*(2/sqrt(10))
upp <- (mu_none - mu_low) + (1/sqrt(2))*q*MSE*(2/sqrt(10))
low
upp
```

ii) Confidence Interval between means: intermediate vs none

```{r}
low <- (mu_none - mu_int)-(1/sqrt(2))*q*MSE*(2/sqrt(10))
upp <- (mu_none - mu_int) + (1/sqrt(2))*q*MSE*(2/sqrt(10))
low
upp
```

iii) Confidence Interval between means: high vs none

```{r}
low <- (mu_none - mu_high)-(1/sqrt(2))*q*MSE*(2/sqrt(10))
upp <- (mu_none - mu_high) + (1/sqrt(2))*q*MSE*(2/sqrt(10))
low
upp
```


iii) As no confidence interval includes zero, we can say with a confidence level of 95% and even when we adjust that level and get a stricter result with both methods, all of the differences are significant.

The confidence intervals we get are very similar in the cases of Bonferroni and Tukey´s method: in general the intervals are not very wide, and they have a similar range. 

## Exercise 2


```{r}
A <- c(324,275,349,604,566,810,340,295,357,580,344,655,380,503,314)
B <- c(558,108,291,863,303,640,358,503,646,689,250,540,630,190)
A_B <- c(A,B)
grouping  <- factor(c(rep('A',length(A)), rep('B', length(B))))
data <- data.frame(Group = grouping, Value = A_B)
```

First, let´s see if they behave normal or not.

```{r}
plot(density(A))
plot(density(B))
```

Let´s perform a Shapiro.Wilkin test, to test our null hypothesis of our distribution following a gaussian.

```{r}
shapiro.test(A)
shapiro.test(B)
```

First, we reject the null hypotesis of Group A having a normal distribution. Nevertheless, when we see the p-value of the second group (group B), it does not behave gaussian. When inspecting the density plots, we see that the group B does not seem to have a skewed distribution, as we can assume normality.

I will perform the Mann-whitney test, that ia non-parametric test, given that we can not assume normality across both groups.

```{r}
wilcox.test(Value ~ Group, data=data )
```

As we can see, the p-value of the test is fairly large. This tells us that we do not have enough evidence to reject the null hypothesis that sais that there is no difference between the medians in the population, for each group.

Let´s see what happens when we study this issue with a resampling method:

```{r}
wilcox_test(Value ~ Group, data=data,distribution = approximate(nresample =1000))
```
Again, our p-value is larger than zero, meaning that at a significance level of 0,05, we can not reject the null hypothesis that claims that both distributions are identical (thus, their medians are the same).


## Exercise 3

This is a categorical data problem. Now we want to do a comparison between to independent groups, the one that got conventional therapy and other that got  alternative therapy, and check the followeing hypotesis test:

Ho) The relapses and the groups are independent (there is homogeneity in the distribution of the relapses among the comparison groups).
H1) There is a difference in the distribution of the relapses among the two groups

The test is called Chi-squared test of independance:

```{r}
therapy = matrix ( c (2 ,8 ,21 ,16) ,
nrow = 2 ,
dimnames = list ( " Treatment " = c ( " Conventional " , " Alternative " ) ,
" Relapses " = c ( " Yes " , " No " )))
therapy
```


As the test statistic works only for big samples, I need to check that the expected frequencies are at least 5.

```{r}
chisq.test(therapy)$expected
```

As we can see, there is one cell that the expected count is smaller than 5. Nevertheless, it is very close to 5. For this reason I will do the analysis both with a chi-square test and with a Fisher Exact test.

```{r}
chisq.test(therapy)
```
As we can see, the p-value is larger than 0.05 and that means we can not reject the null hypothesis that sais that the treatment has no effect in the outcome. 

Let´s see what happens when we check with a resampling method:

```{r}
chisq.test (therapy,simulate.p.value = TRUE , B =1000)
```
Again, our p-value is larger than 0.05 and that makes us think that we can not reject the null hypothesis. It seems that at this level of significance (0.05) the two therapies are equally effective.

As stated before, some of the values when calculating the expected count in the contingency table were below 5. Even though when we round it, it gets to 5, we will still perform the Fisher Exact Test to be sure about our conclusions.


With Fisher Exact test:

```{r}
fisher.test(therapy)
```

Finally, this confirms our conclusions. There is no significant difference between the treatments in the final outcome at a significance level of 0.05.

## Exercise 4

a) First, we build a contingency table with our two categorical variables.

```{r}
exerc_4 = matrix ( c (17 ,298 ,230 ,428) ,
nrow = 2 ,
dimnames = list ( " Seatbelt " = c ( " Yes " , " No " ) ,
" Head_Injury " = c ( " Yes " , " No " )))
exerc_4
```

b) By only observing the table, we can not say that wearing a seatbelt really makes a difference. From the people who uses seatbelt, only a few got a head injury. Actually, only  7% of the people who wears a seatbelt gets a head injury.
From the drivers that did not wear a seatbelt, more than 40% of them got a head injury. So from this simple analysis, we could think that there is an effect of using (or not) the seatbelt in the result of getting (or not) a head injury.

c) Let´s check the expected counts for the contingency table.

```{r}
chisq.test(exerc_4)$expected
```

As we can see, all of the expected counts are larger than 5. Then, we can easily perform a chi-square test to check if wearing a seatbelt affects head injury variable. 

```{r}
chisq.test(exerc_4)
```
We reject the null hypothesis that claims that the head injury and the seatbelt use are independent, as the p-value is very small. In other words, we reject the null hypotesis that sais that there is homogeneity in the distribution of the outcome among the two groups.

Now, we can do the same but with a resampling method.

```{r}
chisq.test(exerc_4,simulate.p.value = TRUE , B =1000)
```
Also with the resampling method, we reject the null hypothesis, and can say that there seems to be an effect in the use of seatbelts and the head injuries.


## Exercise 5

```{r}
exerc_5 = matrix ( c (6,2,17,13,37,44) ,
nrow = 2 ,
dimnames = list ( " Treatment " = c ( " Active_Drug " , " Placebo " ) ,
" Ocluar_Discomfort " = c ( "2","3","4")))
exerc_5
```

Now, let´s check if the expected cell counts exced 5.

```{r}
chisq.test(exerc_5)$expected
```

As we can see, the condition of expected counts greater or equal than 5 is not satisfied, so this time, we will use the Fishers Exact Test. 

With this test, we check if the probability that a person was on a real treatment, given that his discomfort was a certain one, and the probability that a person was in the placebo treatment, given that his discomfort is other. 

This way, we will perform two-sided tests for every combination of groups. In this sense, we will perform 1 Fisher Exact tests, comparing the effect of the treatment in the outcome of ocular discomfort. 

```{r}
fisher.test(exerc_5[1:2,1:3])
```

As we can see, when we do a simple analysis of the three groups together, we again can not reject the null hypothesis, as the p-value is large. Therefore, we can say that there is no difference between the groups in ocular discomfort, at that significance level.

Now, let´s see what happens when we use a resampling method.

```{r}
chisq.test(exerc_5[1:2,1:3] , simulate.p.value = TRUE , B =1000)
```

Again, the p-value of our test is large, confirming what we were concluding before. It seems that there is no significant difference between the placebo patients and treated patients.

## Exercise 6

For this exercise, we have two groups, those who received standard and supplemental treatments. Our data is categorical, where the volumes of surviving and deceased patients for each group are recorded with counts for different causes of death. We would like to assess for an association between the treatment group (angioplasty) and outcome (death). Since we are working with categorical data within a comparison group framework, we can work with the Chi squared test, but we must check the expected counts for our categories are sufficiently high above 5. 

```{r, echo = FALSE}
data <- matrix(c(23, 25, 45, 51, 17, 19, 1064, 1043, 1149, 1138), nrow = 2)

# chi squared test for independence
chisq.test(data)$expected
```

We verify the expected counts of our columns with the chisq.test() function and confirm that we have sufficiently high expected counts for all our categories. We can proceed to use the Chi squared test for independence between the surviving and total patients. Independence between the two groups (control and treatment) would imply the homogeneity of the outcome distribution (death or survival). For our case, we will consider the multiple outcomes with respect to different causes of death.

```{r, echo = FALSE}
# our expected counts are above 5, we can use the chi squared test
chisq.test(data[, 1:4])
```

Our Chi squared test gives us a p-value of 0.867, so we do not reject the Null hypothesis, which implies we have independence between the treatment and control groups. Thus, we do not have statistical evidence to support the effect of supplemental treatment on the survival outcome. We can also run Fisher's Exact Test, which directly interpolates the probabilities in the table. 

```{r, echo = FALSE}
stats::fisher.test(data[, 1:4])
```

The results from Fisher's test point to the same result, we do not reject the Null Hypothesis, which states that the outcome probabilities for the two groups are equal, thus we have evidence for homogeneity across treatment and control groups. 

## Exercise 7

For this exercise, we are comparing the volumes of HgbA1c before and after one year of treatment. We will use a comparison of means procedure to assess the effect of treatment.

```{r, echo = FALSE}
data <- matrix(c(1, 6.7, 7.0, -0.3,
                 2, 7.4, 7.4, 0,
                 3, 9.2, 8.6, 0.6,
                 4, 9.6, 8.1, 1.5,
                 5, 7.4, 6.8, 0.6,
                 6, 8.1, 7.0, 1.1,
                 7, 10.8, 8.5, 2.3,
                 8, 7.1, 7.7, -0.6,
                 9, 7.9, 9.7, -1.8,
                 10, 10.8, 7.7, 3.1), ncol = 4, byrow = TRUE)

# the average HgbA1c level after one year is 0.65 higher than the previous
t.test(data[,2], data[,3], paired = TRUE)
# we have a p-value of 0.186, we do not reject H0
# this implies that the means are not equal
```

We use the t-test to compute the difference of means between the paired groups, since we are measuring the HgbA1c levels for the same individual in two points in time. We run the t-test on the hypothesis of the difference in means, which gives us a p-value of 0.186, which is sufficiently high to not reject the null hypothesis. This provides evidence that the difference of means is equal to zero, therefore the means of the two groups are the same. We see that the 95% confidence interval does include zero, so it is statistically feasible that the difference is very small or negligible. Thus, the effect, while observable, may not be statistically significant. 

```{r, echo = FALSE}
paired.perm.test(data[,2] - data[,3])
# similar p-value from permutation tests, we do not reject H0
# difference of means is no zero
```

Since we are working with a small sample, we can use permutation tests to give us a more realistic p-value. In this case, we see that our p-value is much higher at 0.191, most likely due to the small sample size. Thus, we have evidence to not reject the null hypothesis and claim that the difference of means, given our small sample size and significance level, is zero and that the treatment does not assert an effect on the outcome.

## Exercise 8

In this exercise, we have a table detailing the caloric intake and oxygen consumption of a set of 10 children. We would like to assess the independence of these two variables. We are working with unpaired, numerical data in a small sample. We should first check for normality in order to select the most relevant test of independence.

```{r, echo = FALSE}
data <- matrix(data = c(50, 70, 90, 120, 40, 100, 150, 110, 75, 160, 
                        7, 8, 10.5, 11, 9, 10.8, 12, 10, 9.5, 11.9), ncol = 2)
x = data[,1] 
y = data[,2]
# test for normality
shapiro.test(data)$p.value
# low p-value of 0.0018
# reject H0, we conclude the data is not normally distributed
# we should use a non parametric test
```

The Shapiro-Wilk normality test gives us a very small p-value of 0.0018, which provides evidence for us to reject the normality assumption. Now, we will use correlation tests to assess for dependence between the two variables. We can use the Kendall and Spearman correlation tests, which are non-parametric and do not rely on a normality assumption.

```{r, echo = FALSE}
# correlation tests 
cor.test(data[,1], data[,2], method ="kendall")
cor.test(data[,1], data[,2], method ="spearman")
# very high sample correlations of 0.879 and 0.915, we see a strong linear dependence between the two variables
```

We run the Kendall and Spearman correlation tests, which provide very low p-values and high sample correlation coefficients. This provides evidence to reject the Null Hypothesis that the true correlation coefficient is equal to zero. However, our sample is very small, so it would be best to run permutation tests to confirm our results. We run both correlation tests using permutations and see that our p-values have increased, but are still sufficiently small to reject the Null Hypothesis of zero correlation.

```{r, echo = FALSE}
perm.relation(x = x, y = y, method = "spearman", R = 1000)$p.value
perm.relation(x = x, y = y, method = "kendall", R = 1000)$p.value
```

We plot the two variables and see that there is indeed a very strong positive association between the two variables, which our statistical tests confirmed. We can conclude that we do not have independence and have strong evidence for positive association.

```{r, echo = FALSE}
plot(x, y, xlab = "Calorie Intake", ylab = "Oxygen", main = "Plot of Calorie Intake vs. Oxygen")
```

## Exercise 9

For this exercise, we have a table recording the diastolic blood pressure of 40 patients. Blood pressure was measured consecutively at five months, where half the patients underwent treatment A and the other treatment B. We would like to assess for any statistical evidence that points to a difference in outcome in terms of blood pressure.

We are working with paired numerical data, since repeated measurements were taken for each patient. We must assess for normality for the two groups using the Shapiro-Wilk test. 

```{r, echo = FALSE}

data <- read.delim("dbp.txt", header = TRUE, sep = "")

# test for normality
shapiro.test(data$DBP[data$TRT=="A"])
shapiro.test(data$DBP[data$TRT=="B"])
# we see that our two groups are not normally distributed
```

The Shapiro-Wilk test rejects the Null Hypothesis for normality for both groups, so we should proceed with a non-parametric test to assess the homogeneity in outcome distribution for the two groups. We begin with the difference in means for the two groups, which is 4.41. Specifically, we have that the mean blood pressure for treatment B is 4.41 units higher than for treatment A. 

```{r, echo = FALSE}
# BDP of treatment A vs treatment B
# difference of means
t.test(data$DBP[data$TRT=="A"], data$DBP[data$TRT=="B"],  paired = TRUE)$estimate
# we see that the average BDP for patients with treatment A is about 5 lower than for treatment B
```

Given this observed difference in means, we should test for statistical significance using the Wilcox Mann Whitney test.  

```{r, echo = FALSE}
# test for difference in distribution
wilcox.test(data$DBP[data$TRT=="A"], data$DBP[data$TRT=="B"], paired = TRUE)
# low p value of 4.009e-08 implies that DBP for treatments A and B follow different distributions
```

The Mann Whitney test gives us a very low p-value, so we reject the Null Hypothesis, which provides evidence that the outcome distributions of treatments A and B differ by some location shift. Thus, the difference in means is statistically relevant. We can also run a paired t-test, even though this is sub-optimal given the non-gaussian shape of the data.

```{r, echo = FALSE}
t.test(data$DBP[data$TRT=="A"], data$DBP[data$TRT=="B"], paired = TRUE)
# small p-value from t test implies a non zero difference in means for the two treatments
```

The t-test confirms our observation that the difference of means is non-zero. We can now consider the difference between the two groups over time. We plot the data and observe that the two groups follow distinct distributions, where the blood pressure decreases quasi-linearly for both groups over time, but treatment A has a higher rate of decrease. We also notice that the variability of blood pressure for treatment A increases over time, but stays stable for treatment B. We should note that this heterocedasticity may sabotage our significance tests.

```{r, echo = FALSE}
ggplot()+ aes(x = data$month, y= data$DBP, color = data$TRT) + geom_point() + 
  labs(title = "Treatments A and B over time", x = "Month", y = "DBP", color = "Treatment")
# our plot shows that DBP decreases over time for both treatments, but treatment A has a more negative gradient
# decrease in DBP appears linear with relation to time for both treatments
# we see increased variance with time for treatment A, but homocedasticity for treatment B
```

To address this issue of heterocedasticity, we compute the differences in means for the two groups at each time and run a sequence of t-tests for each month. Our results confirm that the difference in means increases over time, approximately doubling at each month. We run a t-test for each month and our resulting p-values are very low, which provides evidence that the difference in means at each month are statistically relevant, despite the increasing variability of treatment A. 

```{r, echo = FALSE}
# difference in means over time
difference_mean <- c(1:5)
t.test_values <- c(1:5)
for (i in 1:5) {
  difference_mean[i] <- mean(data$DBP[data$month == i&data$TRT=="A"]) - mean(data$DBP[data$month == i&data$TRT=="B"]) 
  t.test_values[i] <- t.test(data$DBP[data$month == i&data$TRT=="A"], data$DBP[data$month == i&data$TRT=="B"])$p.value
}
-difference_mean
t.test_values
```

We have the difference of means plotted below across the five months.

```{r, echo = FALSE}
# our t test p-values imply a non zero difference in means for each group
# we see that the difference of means between the two treatment groups increases with time at a fairly constant rate
plot(-difference_mean, main = "Difference of means of treatment", ylab = "Difference")
```

## Exercise 10

To begin with, we build the contingency matrix, given that our variable is categorical.

```{r}
exerc_10 = matrix ( c (13 ,157,170 ,24 ,505,529,37,622,699) ,
nrow = 3 ,
dimnames = list ( " Control " = c ( " Yes " , " No ", "Total" ) ,
" Hazard " = c ( " Yes " , " No " , "Total")))
exerc_10
```



```{r}
rel_freq <- exerc_10/699
rel_freq
```

This is a case of paired data. We state this, because if we would put the data into a data frame, we would see that each observation has two different categorical variables: "Control" (1 or 0), "Hazard" (1 or 0). This way, each observation has two values (one for each dichotomical variable) and they can not be separated, but have to be taken into account together for each observation.

This way, we can perform a McNemar’s test:


```{r}
mcnemar.test ( exerc_10[1:2,1:2] , correct = FALSE )
```

We rejet independence between the two groups, meaning that we see there is a difference in head injuries, depending on what group each observatio belongs to.

Let´s see what happens when we use a resampling method:

```{r}
mh_test(as.table(exerc_10[1:2,1:2]), distribution = approximate(nresample =10000))
```

Again, we get a p-value that is very small, meaning that we can reject the null hypothesis (with 95% confidence) of independence between the two groups. This way, we can say that the head injuries are not the same for everyone, but depends on which group each person is.

## Exercise 11

There are many ways in which we can define the False Descovery Rate. It is the probability that a null hypothesis is true, when it has been rejected by the test statistic. Rejection of the Null Hypothesis often implies significance. For example, H0 may indicate a coefficient is non-zero. Thus, a false rejection of the Null Hypothesis implies a false significance. Finally, we can also say that there is the expected proportion of false positives from all the tests that were considered statistically significative. The FDR has a very easy interpretation: if in an analysis we are told that the FDR is of 20%, we can tell that maximum a 20% of the results considered significative are false positives actually.

Once thing we can do is to try and control the false discovery rate, by establishing a significance limit for a set of tests given that, from all the  tests that were considered significant, the proportion of null hypothesis (false positives) do not excedes a certain value.

We will describe two different ways to control by False Discovery Rate: Benjamini & Hochberg and q-Value procedure.

### Benjamini & Hochberg

This is the first approximation to control for the FDR. According to this method, in a research where there are "n" tests, if we want the FDR not to exced certain threshold denoted by "d", we would have to:

1) Order the n test from smallest to largest p-value (p_1,p_2,....p_n)
2) Assign ranks to the p-values. For instane, rank 1 for the smallest p-value, rank 2 to the second, and so on.
3) Calculate each individual p-value’s Benjamini-Hochberg critical value, using the formula (i/n)d, where:

i = the individual p-value’s rank,
n = total number of tests,
d = the false discovery rate (a percentage, chosen by you).

4) Compare your original p-values to the critical B-H from Step 3; find the largest p value that is smaller than the critical value.

This way of controlling by FDR becomes extremly useful, as the number of comparison increases. Although we can still use it for analysis with not so many comparisons, when we do have a lot of comparisons to make, this becomes extremely useful.

On the other hand, q-values is another way of dealing with multiple testing and FDR. We can define it as the minimum FDR that can be attained when calling a specific feature "significant". So, in a study where a specific gen has a p-value of 0.020, means that 2% of the genes that show a p-value at least as small as this gene, are false positives.

In gneral, when we take a p-value of 0.05, it means that % of our tests will result in false positives. In contrast, a q-value of 5% means that 5% of significant results will be fasle positives. In other words, the q-value does not take into account all the test that we perform, but only take into account the tests that are below certain threshold that we choose (for exmample 10% or 5%).


Example:

Imagine we have a project about happiness, and we want to know what variables are significant in determiningcertain level of happiness. as we have 15 variables, we get 15 different p-values. Let´s say we want to control our consideration of significance by FDR with Benjamini & Hochberg.

```{r}
Input = ("
  Variable            pvalues
  Depression        .286
  Kids              .56
  Obesity           .002
  Smoking        .040
  Wage            .34
  Marriage        .21
  Religion          .68
  St_relationship   .093
  Eco_security      .12
  Sex                .47
  heal_food           .042
  Park_access        .026
  Shelter           .83
  Park_acc        .0789
  Pollution       .42
")
results = read.table(textConnection(Input), header = TRUE)
results
```

Now, I can order the p-values from smaller to bigger:

```{r}
results = results[order(results$pvalues),]
results
```

Now, let´s perform the BH method to check the adjusted p-values:

```{r, echo=TRUE, warning=FALSE, message=FALSE}
results$BH = p.adjust(results$pvalues,method = "BH")
results
```

As we can see, when we do not adjust by any method, we reject the first 4 null hypothesis. But when we adjust through Benjamini and Hogdberg we only reject one null hypothesis, the one related to obesity. 

Now, let´s see what happens when we calculate the q-values and add them to the table:

```{r}
if (!requireNamespace("BiocManager", quietly = TRUE))
    install.packages("BiocManager")
library(qvalue)
qobj<-qvalue(p=results$pvalues, fdr.level=0.05, lambda=0, pi0.method="smoother")
results$qvalues <- qobj$qvalues
results
```

First, the values are the same for BH and q-values. But they are useful in very different ways.

For instance, if we take the variable "Wage" that has a q-value of 0.51 we can say that we should expect 51% of the variables with a q-value lower than this to be false positives. 







