---
title: "Biostatistics - Task  1"
author: "Mauricio Marcos Fajgenbaun"
date: "13/5/2021"
output: html_document
---


```{r}
library( vcd )
library( dplyr )
library( mosaic )
```


## Exercise 1

First, we need to create a data frame with the values of the table.

```{r}
x <- data.frame("Density" = c("None","Low","Intermediate","High"), "Mean_Seagrass" = c(34.81,33.13,28.33,15.00))
x
x$Density <- as.factor(x$Density);
x$Density
```

a) Three Bonferroni adjusted confidence interval, comparing each parcel with the parcel that had no oysters introduced.

In general, a confidence interval testing equal mean between groups can be performed as follows:

```{r}
library(knitr)
```
$(\hat{\mu_1}-\hat{\mu_2})\pm t_1_-$

```{r}
mu_none <- 34.81
mu_low <- 33.13
mu_int <- 28.33
mu_high <- 15.00
MSE <- 220.94
n <- 10
N <- 20
k = 2
alpha <- 0.05
comp <- 3
alpha_bonf <- alpha/comp
t <- pt(1-alpha_bonf/2,N-k)
```

Now, we can compute the significance level, adjusted by the bonferroni method: 

```{r}
bonf_confid <- (1-(alpha_bonf*2))
bonf_confid
```

i) Confidence Interval between means: low vs none

```{r}
low <- (mu_none-mu_low) - t*sqrt((1/n)+(1/n))
upp <- (mu_none-mu_low) + t*sqrt((1/n)+(1/n))
low
upp
```
This means that a bonferroni adjusted confidence interval at 96.66667% of confidence for the mean difference between the control parcel and the one with low density of oysters is: (1.3075 , 2.0524).

Let´s repeat the same proceadure for the other two comparisons:


ii) Confidence Interval between means: intermediate vs none

```{r}
low <- (mu_none-mu_int) - t*sqrt((1/n)+(1/n))
upp <- (mu_none-mu_int) + t*sqrt((1/n)+(1/n))
low
upp
```

iii) Confidence Interval between means: high vs none

```{r}
low <- (mu_none-mu_high) - t*sqrt((1/n)+(1/n))
upp <- (mu_none-mu_high) + t*sqrt((1/n)+(1/n))
low
upp
```

b) Repeat the previous item with the Tukey’s Honest Significant Difference
`
Now, instead of using a t-statistic, we will be using a studentized range distribution. The confidence interval with the Tukey´s method is as follows:

https://www.itl.nist.gov/div898/handbook/prc/section4/prc471.htm (formula de CI)

```{r}
q <- ptukey(0.05,4,36)
q
```

i) Confidence Interval between means: low vs none

```{r}
low <- (mu_none - mu_low)-(1/sqrt(2))*q*MSE*(2/sqrt(10))
upp <- (mu_none - mu_low) + (1/sqrt(2))*q*MSE*(2/sqrt(10))
low
upp
```

ii) Confidence Interval between means: intermediate vs none

```{r}
low <- (mu_none - mu_int)-(1/sqrt(2))*q*MSE*(2/sqrt(10))
upp <- (mu_none - mu_int) + (1/sqrt(2))*q*MSE*(2/sqrt(10))
low
upp
```

iii) Confidence Interval between means: high vs none

```{r}
low <- (mu_none - mu_high)-(1/sqrt(2))*q*MSE*(2/sqrt(10))
upp <- (mu_none - mu_high) + (1/sqrt(2))*q*MSE*(2/sqrt(10))
low
upp
```

iii) As no Confidence interval includes zero, we can say with a confidence level of 95% and even when we adjust that level and get a stricter result with both methods, all of the differences are significant.

## Exercise 2


```{r}
A <- c(324,275,349,604,566,810,340,295,357,580,344,655,380,503,314)
B <- c(558,108,291,863,303,640,358,503,646,689,250,540,630,190)
```

First, let´s see if they behave normal or not.

```{r}
plot(density(A))
plot(density(B))
```

Let´s perform a Shapiro.Wilkin test, to test our null hypothesis of our distribution following a gaussian.

```{r}
shapiro.test(A)
shapiro.test(B)
```

First, we reject the null hypotesis of Group A having a normal distribution. Nevertheless, when we see the p-value of the second group (group B), it does not behave gaussian. When inspecting the density plots, we see that the group B does not seem to have a skewed distribution, as we can assume normality.



## Exercise 3

This is a categorical data problem. Now we want to do a comparison between to independent groups, the one that got conventional therapy and other that got  alternative therapy, and check the followeing hypotesis test:

Ho) The relapses and the groups are independent (there is homogeneity in the distribution of the relapses among the comparison groups).
H1) There is a difference in the distribution of the relapses among the two groups

The test is called Chi-squared test of independance:

```{r}
therapy = matrix ( c (2 ,8 ,21 ,16) ,
nrow = 2 ,
dimnames = list ( " Treatment " = c ( " Conventional " , " Alternative " ) ,
" Relapses " = c ( " Yes " , " No " )))
therapy
```


As the test statistic works only for big samples, I need to check that the expected frequencies are at least 5.

```{r}
chisq.test (therapy)$ expected
```

As we can see, there is one cell that the expected count is smaller than 5. Nevertheless, it is very close to 5. For this reason I will do the analysis both with a chi-square test and with a Fisher Exact test.

```{r}
chisq.test(therapy)
```
As we can see, the p-value is larger than 0.05 and that means we can not reject the null hypothesis that sais that the treatment has no effect in the outcome. 

Let´s see what happens when we check with a resampling method:

```{r}
chisq.test (therapy,simulate.p.value = TRUE , B =1000)
```
Again, our p-value is larger than 0.05 and that makes us think that we can not reject the null hypothesis. It seems that at this level of significance (0.05) the two therapies are equally effective.

Asstated before, some of the values when calculating the expected count in the contingency table were below 5. Even though when we round it, it gets to 5, we will still perform the Fisher Exact Test to be sure about our conclusions.


With Fisher Exact test:

```{r}
fisher.test(therapy)
```

Finally, this confirms our conclusions. There is significant difference between the treatments in the final outcome at a significance level of 0.05.

## Exercise 4

a) First, we build a contingency table with our two categorical variables.

```{r}
exerc_4 = matrix ( c (17 ,298 ,230 ,428) ,
nrow = 2 ,
dimnames = list ( " Seatbelt " = c ( " Yes " , " No " ) ,
" Head_Injury " = c ( " Yes " , " No " )))
exerc_4
```

b) By only observing the table, we can not say that wearing a seatbelt really makes a difference. From the people who uses seatbelt, only a few got a head injury. Actually, only  7% of the people who wears a seatbelt gets a head injury.
From the drivers that did not wear a seatbelt, more than 40% of them got a head injury. So from this simple analysis, we could think that there is an effect of using (or not) the seatbelt in the result of getting (or not) a head injury.

c) Let´s check the expected counts for the contingency table.

```{r}
chisq.test(exerc_4)$expected
```

As we can see, all of the expected counts are larger than 5. Then, we can easily perform a chi-square test to check if wearing a seatbelt affect head injury variable. 

```{r}
chisq.test(exerc_4)
```
We reject the null hypothesis that claims that the head injury and the seatbelt use are independent, as the p-value is very small. In other words, we reject the null hypotesis that sais that there is homogeneity in the distribution of the outcome among the two groups.

Now, we can do the same but with a resampling method.

```{r}
chisq.test(exerc_4,simulate.p.value = TRUE , B =1000)
```

Also with the resampling method, we reject the null hypothesis, and can say that there seems to be an effect in the use of seatbelts and the head injuries.


## Exercise 5

```{r}
exerc_5 = matrix ( c (6,2,17,13,37,44) ,
nrow = 2 ,
dimnames = list ( " Treatment " = c ( " Active_Drug " , " Placebo " ) ,
" Ocluar_Discomfort " = c ( "2","3","4")))
exerc_5
```

Now, let´s check if the expected cell counts exced 5.

```{r}
chisq.test(exerc_5)$expected
```

As we can see, the condition of expected counts greater or equal than 5 is not satisfied, so this time, we will use the Fishers Exact Test. 

With this test, we check if the probability that a person was on a real treatment, given that his discomfort was a certain one, and the probability that a person was in the placebo treatment, given that his discomfort is other. 

This way, we will perform two-sided tests for every combination of groups. In this sense, we will perform 3 Fisher Exact tests, comparing the effect of the treatment in the outcome of ocular discomfort. 

First, between discomfort "1" and "2":

```{r}
fisher.test(exerc_5[,1:2])
```

As we can see, the difference between the two proportions is not significant.

Now, between "1" and "3":

```{r}
fisher.test(exerc_5[,1:3])
```

Again the difference does not appear to be significant.

Finally, between "2" and "3":

```{r}
fisher.test(exerc_5[,2:3])
```

And again, the proportions are not significantly different.

As in the three tests, we get large p-values, we can not reject the null hypothesis of same proportions. This means that we can not say that there is a difference in ocular discomfort between active drug and placebo patients.

```{r}
fisher.test(exerc_5[1:2,1:3])
```

As we can see, when we do a simple analysis of the three groups together, we again can not reject the null hypothesis, as the p-value is large. Therefore, we can say that there is no difference between the groups in ocular discomfort.

Now, let´s see what happens when we use a resampling method.

```{r}
chisq.test(exerc_5[1:2,1:3] , simulate.p.value = TRUE , B =1000)
```

Again, the p-value of our test is large, confirming what we were concluding before. It seems that there is no significant difference between the placebo patients and treated patients.

## Exercise 11

There are many ways in which we can define de False Descovery Rate. 
One way of defining it is to say that it represents the expected proportion of tests in which the null hypothesis is true, from all the tests that have been considered significative. So it is the probability that a null hypothesis is true, when it has been rejected by the test statistic. Or, in the same sense, from all the tests that have been significative



De entre todos los test considerados significativos, el FDR es la proporción esperada de esos test para los que la hipótesis nula es verdadera.
Es la proporción de test significativos que realmente no lo son.
La proporción esperada de falsos positivos de entre todos los test considerados como significativos.

11. Resume the main ideas about the False Discovery Rate (FDR). Explain the
Benjamini-Hochberg and the q-Value procedures. Show examples of application with R
with a comparison between both methods.
